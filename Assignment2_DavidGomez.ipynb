{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajfsk_7JY3E"
      },
      "source": [
        "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be first graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). However, any given item may be worth 4 or 8 points; if an item is worth 8 points, you need to accordingly scale the 0-4 grade.\n",
        "\n",
        "\n",
        "The total score must be re-scaled to 100. That should apply to all future assignments so that Canvas assigns the same weight on all assignments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArgW_Vq-uTh"
      },
      "source": [
        "# Assignment 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation Steps\n"
      ],
      "metadata": {
        "id": "fKHAiVXNz-2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We will work with this [mystery dataset](https://drive.google.com/open?id=1WLnWBThCYZ25pReI5DCwk2bgDaCrJxI_&authuser=ikoutis%40njit.edu&usp=drive_fs) that you can download and place to your google drive. You can then put it somewhere on your google drive and bring it into your Colab by following the steps in the following cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "5xtT_YAhmmXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "#mat = scipy.io.loadmat('/content/gdrive/Shareddrives/graph-modification/data/mysteryDataset.mat')"
      ],
      "metadata": {
        "id": "5AMaVk7jmn_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c546d7-e6a1-4418-fafa-b910b5b1bdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file contains\n",
        "\n",
        "* Two matrices $X$ and $X_1$ of numerical features. These datasets have the same dimensions (169343x80) but they are different.\n",
        "* An array $y$ of labels, ranging from 0-39.\n",
        "* The indices $otrain$ of a training set. These indices tell you what rows of the arrays $X,X_1,y$ correspond to the training points. You can use these to make two different training sets $(X[train], y[train])$ and $(X_1[train], y[train])$\n",
        "* Similarly, it contains the indexes for a validation and a test set, $ovalid$ and $otest$ respectively.\n",
        "\n",
        "The following cell shows how to access these arrays and assign them to local numpy objects."
      ],
      "metadata": {
        "id": "AsLbaghZrO-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat = scipy.io.loadmat('mysteryDataset.mat')"
      ],
      "metadata": {
        "id": "HQ0Oyva2F3fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is an example\n",
        "#ft = mat.get('X')"
      ],
      "metadata": {
        "id": "d0jvswNpwsOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "SFD1w69Kv8cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'blue'> Question 1. Import the dataset and conver to torch tensors </font>\n",
        "\n",
        "Your task for this question is to adapt the above preparation steps, import all mentioned variables into numpy arrays, and then transform them to PyTorch tensors.\n"
      ],
      "metadata": {
        "id": "ALSnNeSw0JoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapting the above preparation steps\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import scipy.io\n",
        "\n",
        "# Downloading data\n",
        "mat = scipy.io.loadmat('/content/gdrive/MyDrive/Colab Notebooks/CS677/mysteryDataset.mat')"
      ],
      "metadata": {
        "id": "C20aEPMG0z2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0188fbda-3225-48ad-d989-9c0b7351354d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all mentioned variables into numpy arrays, and then transform them to PyTorch tensors\n",
        "\n",
        "import torch\n",
        "\n",
        "# Converting to tensors\n",
        "for key in ['X','X1','y','otrain','ovalid','otest']:\n",
        "  mat[key] = torch.from_numpy(mat[key])\n",
        "  if key in ['X','X1','y']: mat[key] = mat[key].float()"
      ],
      "metadata": {
        "id": "pAdDfEFYs0H9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning to local variables\n",
        "\n",
        "X = mat['X']\n",
        "X1 = mat['X1']\n",
        "y = mat['y']\n",
        "otrain = mat['otrain']\n",
        "ovalid = mat['ovalid']\n",
        "otest = mat['otest']\n",
        "del mat # Delete the matrix to save memory"
      ],
      "metadata": {
        "id": "VfimdE0auIpQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBjMZF1wGaUp"
      },
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 4)\n",
        "\n",
        "# G[1] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "weZCxabEv-sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'blue'> Question 2. Write a functioning classifier in PyTorch </font>\n",
        "\n",
        "Write code that defines a classification model for the above dataset, and all other functions that are needed for its training. Apply your model on the two datsets $X,X_1$ and report the accuracy. The classifier should operate on the GPU.\n",
        "\n",
        "**Hint:** Re-use code we discussed for the Softmax Regression module."
      ],
      "metadata": {
        "id": "s7I1lmGM0skC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Weights initializer\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "# Accuracy function\n",
        "def accuracy(X,y):\n",
        "  return (model(X.to(device))[0].argmax(axis=1) == y.to(device)[0].reshape(-1)).sum()/y.shape[1]\n",
        "\n",
        "# Creating dataloader\n",
        "def dataloader(X,y,batch_size):\n",
        "  return DataLoader([[X[0][i],y[0][i]] for i in range(X.shape[1])],\n",
        "                    batch_size = batch_size,\n",
        "                    shuffle = True)"
      ],
      "metadata": {
        "id": "na0el-GE1qtY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Model\n",
        "class SoftMaxRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.LazyLinear(40)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "EKK9bbQZu2_y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating train, validation, and test sets\n",
        "\n",
        "X_train, X1_train, y_train = X[otrain], X1[otrain], y[otrain]\n",
        "X_valid, X1_valid, y_valid = X[ovalid], X1[ovalid], y[ovalid]\n",
        "X_test, X1_test, y_test = X[otest], X1[otest], y[otest]"
      ],
      "metadata": {
        "id": "J1P5HktMu5rH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "0cz7JgGLwC4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET X\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1000\n",
        "n_epochs = 500\n",
        "lr = 10e-3\n",
        "\n",
        "# Creating dataloader\n",
        "torch.manual_seed(0)\n",
        "train_iter = dataloader(X_train,y_train,batch_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Assigning to GPU if available\n",
        "\n",
        "model = SoftMaxRegression().to(device) # Model\n",
        "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr) # Optimizer\n",
        "\n",
        "model.apply(init_weights) # Initialize Weights\n",
        "\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  total_loss = 0\n",
        "  for x_batch, y_batch in train_iter:\n",
        "\n",
        "    # Geting batches\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = F.one_hot(y_batch.reshape(-1).long(),40).to(device).float()\n",
        "\n",
        "    model.train() # Puting model into training mode\n",
        "    yhat = model(x_batch) # Get prediction\n",
        "    loss = loss_fn(yhat, y_batch) # Calculate loss\n",
        "\n",
        "    # Updating weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss/batch_size\n",
        "\n",
        "  if not epoch % 50:\n",
        "    model.eval()\n",
        "    print(epoch,total_loss)\n",
        "    print(accuracy(X_train,y_train),accuracy(X_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdFa51nJu-t5",
        "outputId": "fd794444-1d2a-4f5a-aa69-723e370b7a33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 tensor(0.2644, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.1914, device='cuda:0') tensor(0.0788, device='cuda:0')\n",
            "100 tensor(0.2532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.2104, device='cuda:0') tensor(0.0859, device='cuda:0')\n",
            "150 tensor(0.2444, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.2936, device='cuda:0') tensor(0.2610, device='cuda:0')\n",
            "200 tensor(0.2372, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3353, device='cuda:0') tensor(0.3228, device='cuda:0')\n",
            "250 tensor(0.2311, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3403, device='cuda:0') tensor(0.3251, device='cuda:0')\n",
            "300 tensor(0.2259, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3490, device='cuda:0') tensor(0.3286, device='cuda:0')\n",
            "350 tensor(0.2215, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3561, device='cuda:0') tensor(0.3326, device='cuda:0')\n",
            "400 tensor(0.2177, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3652, device='cuda:0') tensor(0.3358, device='cuda:0')\n",
            "450 tensor(0.2143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3712, device='cuda:0') tensor(0.3391, device='cuda:0')\n",
            "500 tensor(0.2114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.3802, device='cuda:0') tensor(0.3451, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "model.eval()\n",
        "print(\"Train Accuracy:\",accuracy(X_train,y_train))\n",
        "print(\"Test Accuracy:\",accuracy(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23kbNz-OvYaF",
        "outputId": "f076f20a-f19c-4926-d6d8-e76993b875db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: tensor(0.3802, device='cuda:0')\n",
            "Test Accuracy: tensor(0.3032, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "gCxSO7EmwHie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET X1\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1000\n",
        "n_epochs = 500\n",
        "lr = 10e-3\n",
        "\n",
        "# Creating dataloader\n",
        "torch.manual_seed(0)\n",
        "train_iter = dataloader(X1_train,y_train,batch_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Assigning to GPU if available\n",
        "\n",
        "model = SoftMaxRegression().to(device) # Model\n",
        "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr) # Optimizer\n",
        "\n",
        "model.apply(init_weights) # Initialize Weights\n",
        "\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  total_loss = 0\n",
        "  for x_batch, y_batch in train_iter:\n",
        "\n",
        "    # Geting batches\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = F.one_hot(y_batch.reshape(-1).long(),40).to(device).float()\n",
        "\n",
        "    model.train() # Puting model into training mode\n",
        "    yhat = model(x_batch) # Get prediction\n",
        "    loss = loss_fn(yhat, y_batch) # Calculate loss\n",
        "\n",
        "    # Updating weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss/batch_size\n",
        "\n",
        "  if not epoch % 50:\n",
        "    model.eval()\n",
        "    print(epoch,total_loss)\n",
        "    print(accuracy(X1_train,y_train),accuracy(X1_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49lp2pBavdvi",
        "outputId": "d91f6121-dd59-4aac-b0ec-4563f5bebc94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 tensor(0.1559, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5143, device='cuda:0') tensor(0.5140, device='cuda:0')\n",
            "100 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5144, device='cuda:0') tensor(0.5168, device='cuda:0')\n",
            "150 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5154, device='cuda:0') tensor(0.5159, device='cuda:0')\n",
            "200 tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5150, device='cuda:0') tensor(0.5192, device='cuda:0')\n",
            "250 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5154, device='cuda:0') tensor(0.5196, device='cuda:0')\n",
            "300 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5156, device='cuda:0') tensor(0.5215, device='cuda:0')\n",
            "350 tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5147, device='cuda:0') tensor(0.5175, device='cuda:0')\n",
            "400 tensor(0.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5160, device='cuda:0') tensor(0.5169, device='cuda:0')\n",
            "450 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5149, device='cuda:0') tensor(0.5185, device='cuda:0')\n",
            "500 tensor(0.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5164, device='cuda:0') tensor(0.5203, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "model.eval()\n",
        "print(\"Train Accuracy:\",accuracy(X1_train,y_train))\n",
        "print(\"Test Accuracy:\",accuracy(X1_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiKKKyY4v1Sq",
        "outputId": "fd628767-ac93-4faa-c75d-77f468a775cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: tensor(0.5164, device='cuda:0')\n",
            "Test Accuracy: tensor(0.4969, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "nWAp_cZxwIkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Q.2\n",
        "\n",
        "Using a simple soft max classifier (lr=10e-3, epochs = 500, batch_size = 1000, no hidden layers), the accuracy was the following:\n",
        "\n",
        "Dataset X:\n",
        "- Train Accuracy: 38.02%\n",
        "- Test Accuracy: 30.32%\n",
        "\n",
        "Dataset X1:\n",
        "- Train Accuracy: 51.64%\n",
        "- Test Accuracy: 49.69%"
      ],
      "metadata": {
        "id": "RA_xW0JywJY2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlICv7kW1qte"
      },
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 8)\n",
        "\n",
        "# G[2] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "CxNLEQhpwKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'blue'> Question 3. Maximize the accuracy on the two datasets </font>\n",
        "\n",
        "Augment your classifier from Question-2 with any number and type of layers you want, with the goal to maximize the **validation** accuracy you achieve on the two datasets. Feel free to use any stopping criterion you want for the training process. The networks for $X$ and $X_1$ do not have be of the same architecture.\n",
        "\n",
        "Show your code, and add a text cell summarizing your idea and findings. Finally apply your models to the **test** set, and report the accuracy. Feel free to discuss your validation accuracy on Canvas. Also please avoid looking at the test set, until the very end.\n",
        "\n",
        "**Rubric**: All complete answers get 8 points, and the **top 5** test accuracies reported get an extra 10\\% in the final quiz."
      ],
      "metadata": {
        "id": "gDmA1ZZL152G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET X\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.LazyLinear(1000),\n",
        "        nn.ReLU(),\n",
        "        nn.LazyLinear(500),\n",
        "        nn.ReLU(),\n",
        "        nn.LazyLinear(40)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "3lF1iUaBwdK8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 500\n",
        "n_epochs = 2000\n",
        "lr = 10e-3\n",
        "\n",
        "# Creating dataloader\n",
        "torch.manual_seed(0)\n",
        "train_iter = dataloader(X_train,y_train,batch_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Assigning to GPU if available\n",
        "\n",
        "model = Net().to(device) # Model\n",
        "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr) # Optimizer\n",
        "\n",
        "model.apply(init_weights) # Initialize Weights\n",
        "\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  total_loss = 0\n",
        "  for x_batch, y_batch in train_iter:\n",
        "\n",
        "    # Geting batches\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = F.one_hot(y_batch.reshape(-1).long(),40).to(device).float()\n",
        "\n",
        "    model.train() # Puting model into training mode\n",
        "    yhat = model(x_batch) # Get prediction\n",
        "    loss = loss_fn(yhat, y_batch) # Calculate loss\n",
        "\n",
        "    # Updating weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss/batch_size\n",
        "\n",
        "  if not epoch % 50:\n",
        "    model.eval()\n",
        "    print(epoch,total_loss)\n",
        "    print(accuracy(X_train,y_train),accuracy(X_valid,y_valid))"
      ],
      "metadata": {
        "id": "ySrVeueW31b_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a0316f-90f3-4e17-f673-ece11b7c2771"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 tensor(0.5338, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5850, device='cuda:0') tensor(0.5184, device='cuda:0')\n",
            "100 tensor(0.4962, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5931, device='cuda:0') tensor(0.5174, device='cuda:0')\n",
            "150 tensor(0.4764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6243, device='cuda:0') tensor(0.5938, device='cuda:0')\n",
            "200 tensor(0.4689, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6295, device='cuda:0') tensor(0.5939, device='cuda:0')\n",
            "250 tensor(0.4596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6273, device='cuda:0') tensor(0.5763, device='cuda:0')\n",
            "300 tensor(0.4565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6429, device='cuda:0') tensor(0.6183, device='cuda:0')\n",
            "350 tensor(0.4514, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6524, device='cuda:0') tensor(0.6189, device='cuda:0')\n",
            "400 tensor(0.4485, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6514, device='cuda:0') tensor(0.6128, device='cuda:0')\n",
            "450 tensor(0.4412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6532, device='cuda:0') tensor(0.6278, device='cuda:0')\n",
            "500 tensor(0.4372, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6493, device='cuda:0') tensor(0.6002, device='cuda:0')\n",
            "550 tensor(0.4348, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6249, device='cuda:0') tensor(0.5495, device='cuda:0')\n",
            "600 tensor(0.4333, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6519, device='cuda:0') tensor(0.6211, device='cuda:0')\n",
            "650 tensor(0.4295, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6531, device='cuda:0') tensor(0.6160, device='cuda:0')\n",
            "700 tensor(0.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6540, device='cuda:0') tensor(0.6043, device='cuda:0')\n",
            "750 tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6491, device='cuda:0') tensor(0.6016, device='cuda:0')\n",
            "800 tensor(0.4259, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6639, device='cuda:0') tensor(0.6321, device='cuda:0')\n",
            "850 tensor(0.4224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6665, device='cuda:0') tensor(0.6303, device='cuda:0')\n",
            "900 tensor(0.4203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6619, device='cuda:0') tensor(0.6217, device='cuda:0')\n",
            "950 tensor(0.4209, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6541, device='cuda:0') tensor(0.5993, device='cuda:0')\n",
            "1000 tensor(0.4181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6551, device='cuda:0') tensor(0.6098, device='cuda:0')\n",
            "1050 tensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6575, device='cuda:0') tensor(0.6043, device='cuda:0')\n",
            "1100 tensor(0.4143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6665, device='cuda:0') tensor(0.6295, device='cuda:0')\n",
            "1150 tensor(0.4181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6648, device='cuda:0') tensor(0.6225, device='cuda:0')\n",
            "1200 tensor(0.4164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6657, device='cuda:0') tensor(0.6276, device='cuda:0')\n",
            "1250 tensor(0.4154, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6694, device='cuda:0') tensor(0.6333, device='cuda:0')\n",
            "1300 tensor(0.4123, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6684, device='cuda:0') tensor(0.6401, device='cuda:0')\n",
            "1350 tensor(0.4129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6678, device='cuda:0') tensor(0.6129, device='cuda:0')\n",
            "1400 tensor(0.4102, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6722, device='cuda:0') tensor(0.6375, device='cuda:0')\n",
            "1450 tensor(0.4083, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6694, device='cuda:0') tensor(0.6298, device='cuda:0')\n",
            "1500 tensor(0.4087, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6607, device='cuda:0') tensor(0.6025, device='cuda:0')\n",
            "1550 tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6673, device='cuda:0') tensor(0.6135, device='cuda:0')\n",
            "1600 tensor(0.4082, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6733, device='cuda:0') tensor(0.6222, device='cuda:0')\n",
            "1650 tensor(0.4063, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6736, device='cuda:0') tensor(0.6323, device='cuda:0')\n",
            "1700 tensor(0.4056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6756, device='cuda:0') tensor(0.6248, device='cuda:0')\n",
            "1750 tensor(0.4052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6706, device='cuda:0') tensor(0.6279, device='cuda:0')\n",
            "1800 tensor(0.4054, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6761, device='cuda:0') tensor(0.6327, device='cuda:0')\n",
            "1850 tensor(0.4054, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6654, device='cuda:0') tensor(0.6212, device='cuda:0')\n",
            "1900 tensor(0.4043, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6771, device='cuda:0') tensor(0.6370, device='cuda:0')\n",
            "1950 tensor(0.4018, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6679, device='cuda:0') tensor(0.6288, device='cuda:0')\n",
            "2000 tensor(0.4027, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6759, device='cuda:0') tensor(0.6406, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "model.eval()\n",
        "print(\"Train Accuracy:\",accuracy(X_train,y_train))\n",
        "print(\"Test Accuracy:\",accuracy(X_test,y_test))"
      ],
      "metadata": {
        "id": "aYUwCg20wRyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21f2451-6c9b-4056-e28d-2c79668c1b93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: tensor(0.6759, device='cuda:0')\n",
            "Test Accuracy: tensor(0.6362, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "FlUQhZpjwNma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET X1\n",
        "\n",
        "class Net1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.LazyLinear(256),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.ReLU(),\n",
        "        nn.LazyLinear(512),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.ReLU(),\n",
        "        nn.LazyLinear(1024),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.LazyLinear(40)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "HNPHKbc0xAYS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 500\n",
        "n_epochs = 2000\n",
        "lr = 10e-4\n",
        "\n",
        "# Creating dataloader\n",
        "torch.manual_seed(0)\n",
        "train_iter = dataloader(X1_train,y_train,batch_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Assigning to GPU if available\n",
        "\n",
        "model = Net1().to(device) # Model\n",
        "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr) # Optimizer\n",
        "\n",
        "model.apply(init_weights) # Initialize Weights\n",
        "\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  total_loss = 0\n",
        "  for x_batch, y_batch in train_iter:\n",
        "\n",
        "    # Geting batches\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = F.one_hot(y_batch.reshape(-1).long(),40).to(device).float()\n",
        "\n",
        "    model.train() # Puting model into training mode\n",
        "    yhat = model(x_batch) # Get prediction\n",
        "    loss = loss_fn(yhat, y_batch) # Calculate loss\n",
        "\n",
        "    # Updating weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss += loss/batch_size\n",
        "\n",
        "  if not epoch % 50:\n",
        "    model.eval()\n",
        "    print(epoch,total_loss)\n",
        "    print(accuracy(X1_train,y_train),accuracy(X1_valid,y_valid))"
      ],
      "metadata": {
        "id": "WoLQOFpUw8Gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a174275a-3106-4f16-9c82-9a71ad814b07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 tensor(0.6008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5718, device='cuda:0') tensor(0.5347, device='cuda:0')\n",
            "100 tensor(0.5882, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5908, device='cuda:0') tensor(0.5444, device='cuda:0')\n",
            "150 tensor(0.5823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.5956, device='cuda:0') tensor(0.5412, device='cuda:0')\n",
            "200 tensor(0.5792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6006, device='cuda:0') tensor(0.5413, device='cuda:0')\n",
            "250 tensor(0.5759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6031, device='cuda:0') tensor(0.5391, device='cuda:0')\n",
            "300 tensor(0.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6110, device='cuda:0') tensor(0.5433, device='cuda:0')\n",
            "350 tensor(0.5713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6127, device='cuda:0') tensor(0.5442, device='cuda:0')\n",
            "400 tensor(0.5711, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6130, device='cuda:0') tensor(0.5402, device='cuda:0')\n",
            "450 tensor(0.5689, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6087, device='cuda:0') tensor(0.5409, device='cuda:0')\n",
            "500 tensor(0.5695, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6166, device='cuda:0') tensor(0.5402, device='cuda:0')\n",
            "550 tensor(0.5674, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6174, device='cuda:0') tensor(0.5371, device='cuda:0')\n",
            "600 tensor(0.5680, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6175, device='cuda:0') tensor(0.5386, device='cuda:0')\n",
            "650 tensor(0.5670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6193, device='cuda:0') tensor(0.5396, device='cuda:0')\n",
            "700 tensor(0.5662, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6167, device='cuda:0') tensor(0.5375, device='cuda:0')\n",
            "750 tensor(0.5657, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6225, device='cuda:0') tensor(0.5391, device='cuda:0')\n",
            "800 tensor(0.5647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6179, device='cuda:0') tensor(0.5390, device='cuda:0')\n",
            "850 tensor(0.5665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6207, device='cuda:0') tensor(0.5378, device='cuda:0')\n",
            "900 tensor(0.5643, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6200, device='cuda:0') tensor(0.5335, device='cuda:0')\n",
            "950 tensor(0.5648, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6261, device='cuda:0') tensor(0.5387, device='cuda:0')\n",
            "1000 tensor(0.5634, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6228, device='cuda:0') tensor(0.5379, device='cuda:0')\n",
            "1050 tensor(0.5631, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6240, device='cuda:0') tensor(0.5384, device='cuda:0')\n",
            "1100 tensor(0.5635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6251, device='cuda:0') tensor(0.5376, device='cuda:0')\n",
            "1150 tensor(0.5622, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6233, device='cuda:0') tensor(0.5406, device='cuda:0')\n",
            "1200 tensor(0.5627, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6275, device='cuda:0') tensor(0.5329, device='cuda:0')\n",
            "1250 tensor(0.5630, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6214, device='cuda:0') tensor(0.5349, device='cuda:0')\n",
            "1300 tensor(0.5626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6255, device='cuda:0') tensor(0.5364, device='cuda:0')\n",
            "1350 tensor(0.5632, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6321, device='cuda:0') tensor(0.5392, device='cuda:0')\n",
            "1400 tensor(0.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6238, device='cuda:0') tensor(0.5364, device='cuda:0')\n",
            "1450 tensor(0.5621, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6267, device='cuda:0') tensor(0.5384, device='cuda:0')\n",
            "1500 tensor(0.5610, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6293, device='cuda:0') tensor(0.5349, device='cuda:0')\n",
            "1550 tensor(0.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6266, device='cuda:0') tensor(0.5375, device='cuda:0')\n",
            "1600 tensor(0.5636, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6303, device='cuda:0') tensor(0.5367, device='cuda:0')\n",
            "1650 tensor(0.5615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6298, device='cuda:0') tensor(0.5371, device='cuda:0')\n",
            "1700 tensor(0.5626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6313, device='cuda:0') tensor(0.5336, device='cuda:0')\n",
            "1750 tensor(0.5592, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6310, device='cuda:0') tensor(0.5359, device='cuda:0')\n",
            "1800 tensor(0.5599, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6323, device='cuda:0') tensor(0.5370, device='cuda:0')\n",
            "1850 tensor(0.5603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6310, device='cuda:0') tensor(0.5419, device='cuda:0')\n",
            "1900 tensor(0.5594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6263, device='cuda:0') tensor(0.5333, device='cuda:0')\n",
            "1950 tensor(0.5588, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6290, device='cuda:0') tensor(0.5400, device='cuda:0')\n",
            "2000 tensor(0.5600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.6288, device='cuda:0') tensor(0.5361, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "model.eval()\n",
        "print(\"Train Accuracy:\",accuracy(X1_train,y_train))\n",
        "print(\"Test Accuracy:\",accuracy(X1_test,y_test))"
      ],
      "metadata": {
        "id": "pJCOHgoaxVmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1958d0f-430a-4688-b7b4-655757581764"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: tensor(0.6288, device='cuda:0')\n",
            "Test Accuracy: tensor(0.5155, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_________________________________________________"
      ],
      "metadata": {
        "id": "1FnnBUNGxaxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Q.3\n",
        "\n",
        "To check overfitting and performance of the model, the train and validation accuracy were printed out (every 50 epochs). After selecting parameters based on the best validation, the test accuracy was also taken. While dataset X performed worse in Question 2, it was able to perform better here in Question 3. It was able to reach a higher accuracy without overfitting too much.\n",
        "\n",
        "On the otherhand, dataset X1 was much more prone to overfitting and at a faster rate, resulting in a lower accuracy. Even though a lower number of epochs were used and dropout layers were used, it still tended to overfit. Simpler models with less layers, however, tended to underfit.\n",
        "\n",
        "The results for the two datasets are summarized below:\n",
        "\n",
        "Dataset X (lr=10e-3, batch_size=500,epochs=2000, layers=(1000,500), activation=ReLU, dropout_layers=False):\n",
        "- Train Accuracy: 67.59%\n",
        "- Validation Accuracy: 64.06%\n",
        "- Test Accuracy: 63.62%\n",
        "\n",
        "Dataset X1 (lr=10e-4, batch_size=500,epochs=100, layers=(256,512,1024),activation=ReLU, dropout_layers=True):\n",
        "- Train Accuracy: 62.88%\n",
        "- Validation Accuracy: 53.84%\n",
        "- Test Accuracy: 51.55%"
      ],
      "metadata": {
        "id": "Slt6eG8TwOTB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcYS1afK31cS"
      },
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here  (out of 8)\n",
        "\n",
        "# G[3] =\n",
        "#\n",
        "# please justify point subtractions when needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total score\n",
        "max_score = 20\n",
        "final_score = sum(G)*(100/max_score)"
      ],
      "metadata": {
        "id": "EZHVWBIjIuoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}